% train_coarseDOA_fixed_FINAL.m
clear; clc; rng('default');
ti
%% 1) Carrega todos os datasets em 'datasets/'
dataDir     = fullfile(pwd,'datasets');
files       = dir(fullfile(dataDir,'dataset_coarse_SNR*.mat'));
assert(~isempty(files),"Nenhum dataset encontrado em 'datasets/'.");

T_all = []; Y_all = [];
for k = 1:numel(files)
    D     = load(fullfile(files(k).folder,files(k).name),'Tcoarse','Ylabel');
    T_all = cat(4, T_all, D.Tcoarse);
    Y_all = cat(2, Y_all, D.Ylabel);
end
T_all = single(T_all);  Y_all = single(Y_all);

%% 2) Split treino / validação
Ntot  = size(Y_all,2);
perm  = randperm(Ntot);
Ntrain = floor(0.9*Ntot);

trainIdx = perm(1:Ntrain);
valIdx   = perm(Ntrain+1:end);

XTrain = T_all(:,:,:,trainIdx);   YTrain = Y_all(:,trainIdx);
XVal   = T_all(:,:,:,valIdx);     YVal   = Y_all(:,valIdx);
clear T_all Y_all;

fprintf("Total=%d  (treino=%d  validação=%d)\n",Ntot,Ntrain,numel(valIdx));

%% 3) Modelo (Tabela 1)
inputSize  = size(XTrain,1:3);    % [10 10 3]
numClasses = size(YTrain,1);      % 12

layers = [
    imageInputLayer(inputSize,'Normalization','none')

    convolution2dLayer(3,256,'Padding','same')
    batchNormalizationLayer
    leakyReluLayer(0.3)

    convolution2dLayer(3,128,'Padding','same')
    batchNormalizationLayer
    leakyReluLayer(0.3)

    convolution2dLayer(3,64,'Padding','same')
    batchNormalizationLayer
    leakyReluLayer(0.3)

    fullyConnectedLayer(2048)
    reluLayer

    fullyConnectedLayer(512)
    reluLayer

    fullyConnectedLayer(128)
    reluLayer

    fullyConnectedLayer(numClasses)
    sigmoidLayer
];

dlnet = dlnetwork(layerGraph(layers));

%% 4) Hiper-parâmetros
learnRate     = 1e-3;
gradDecay     = 0.9;   % β1
sqDecay       = 0.999; % β2
miniBatchSize = 128;
numEpochs     = 100;
valFreq       = 200;   % iterações
patience      = 5;

% ► Estados do Adam
avgGrad    = [];   % E[g]
avgSqGrad  = [];   % E[g^2]

bestValLoss = inf; patCount = 0; iteration = 0;
numIterEp   = floor(Ntrain/miniBatchSize);

%% 5) Treinamento
for epoch = 1:numEpochs
    permEpoch = randperm(Ntrain);      % embaralha amostras
    for i = 1:numIterEp
        iteration = iteration + 1;
        batchIDs  = permEpoch((i-1)*miniBatchSize + (1:miniBatchSize));

        Xb = dlarray(XTrain(:,:,:,batchIDs),'SSCB');
        Yb = dlarray(YTrain(:,batchIDs),'CB');

        [Yp,grad,lossTr] = dlfeval(@gradFun,dlnet,Xb,Yb);

        % ► chamada correta do adamupdate
        [dlnet,avgGrad,avgSqGrad] = adamupdate( ...
            dlnet,grad,avgGrad,avgSqGrad, ...
            iteration,learnRate,gradDecay,sqDecay);

        % Validação periódica
        if mod(iteration,valFreq)==0
            lossVal = valLoss(dlnet,XVal,YVal,miniBatchSize);
            fprintf('Ep %2d | It %5d | Train %.4f | Val %.4f\n', ...
                    epoch,iteration,lossTr,lossVal);

            if lossVal < bestValLoss
                bestValLoss = lossVal; bestNet = dlnet; patCount = 0;
            else
                patCount = patCount+1;
                if patCount >= patience
                    fprintf('Early-stopping na época %d.\n',epoch);
                    break;
                end
            end
        end
    end
    if patCount >= patience, break; end
end

%% 6) Salva o melhor modelo
save coarseDOA_net.mat bestNet
fprintf('Modelo salvo em coarseDOA_net.mat (ValLoss=%.4f)\n',bestValLoss);

%% ------------------------------------------------------------------------
function [YPred,gradients,loss] = gradFun(net,X,Y)
    YPred = forward(net,X);
    loss  = -mean(Y.*log(YPred+eps)+(1-Y).*log(1-YPred+eps),'all');
    gradients = dlgradient(loss,net.Learnables);
    loss = double(gather(extractdata(loss)));
end

function L = valLoss(net,Xval,Yval,bs)
    N   = size(Yval,2); nIt = floor(N/bs); acc=0;
    for k = 1:nIt
        idx   = (k-1)*bs + (1:bs);
        Xb    = dlarray(Xval(:,:,:,idx),'SSCB');
        Yb    = Yval(:,idx);
        Ypred = predict(net,Xb);
        acc   = acc + double(gather(extractdata( ...
                -mean(Yb.*log(Ypred+eps)+(1-Yb).*log(1-Ypred+eps),'all'))));
    end
    L = acc/nIt;
end
